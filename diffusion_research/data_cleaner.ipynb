{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rising_queries = pd.read_csv(\"./data/rising_quer_all.csv\")\n",
    "top_queries = pd.read_csv(\"./data/top_quer_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "\n",
    "def group_similar_terms(df, threshold=85):\n",
    "    # Create a dictionary to store the groups of similar terms for each column\n",
    "    term_groups = {col: [] for col in df.columns}\n",
    "\n",
    "    for col in df.columns:\n",
    "        for query in df[col]:\n",
    "            found_group = None\n",
    "\n",
    "            for col2 in term_groups.keys():\n",
    "                for group, terms in term_groups[col2]:  # Groups and terms are a tuple. The group is the combined term and the terms are the cloud of terms. \n",
    "                    for term in terms:\n",
    "                        similarity_score = fuzz.token_sort_ratio(query, term)\n",
    "                        if similarity_score >= threshold:\n",
    "                            found_group = group   # Assumes that there is a preexisting group. \n",
    "                            break\n",
    "\n",
    "                    if found_group:\n",
    "                        break\n",
    "\n",
    "            # If no existing group is found, create a new group\n",
    "            # Adds group name to column\n",
    "            if not found_group:\n",
    "                found_group = f'{query}'\n",
    "                term_groups[col].append((found_group, [found_group]))\n",
    "\n",
    "            # Add the query to the found group\n",
    "            else:\n",
    "                term_groups[col].append((found_group, [found_group]))\n",
    "\n",
    "    # Convert the dictionary back to a dataframe\n",
    "    # For each column in df.columns, for each group in term_groups, for each term add it to the column\n",
    "    grouped_data = {col: [term for group, terms in term_groups[col] for term in terms] for col in df.columns}\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "\n",
    "def produce_groups(df, threshold=85):\n",
    "    # Create a dictionary to store the groups of similar terms for the entire dataset\n",
    "    term_groups = defaultdict(list)\n",
    "\n",
    "    def process_row(row):\n",
    "        for col, query in row.items():\n",
    "            found_group = None\n",
    "\n",
    "            for group, terms in term_groups.items():\n",
    "                for term in terms:\n",
    "                    similarity_score = fuzz.token_sort_ratio(query, term)\n",
    "                    if similarity_score >= threshold: \n",
    "                        found_group = group   \n",
    "                        break\n",
    "\n",
    "                if found_group:\n",
    "                    break\n",
    "\n",
    "            # If no existing group is found, create a new group\n",
    "            # Adds query itself to group name.\n",
    "            if not found_group:\n",
    "                found_group = f'Group_({query})'\n",
    "                term_groups[found_group].append(query)\n",
    "\n",
    "            # Add the query to the found group\n",
    "            else:\n",
    "                term_groups[found_group].append(query)\n",
    "\n",
    "    df.apply(process_row, axis=1)\n",
    "\n",
    "    # Convert the dictionary to a group Dictionary\n",
    "    grouped_terms_dict = {group: list(terms) for group, terms in term_groups.items()}\n",
    "\n",
    "    return grouped_terms_dict\n",
    "\n",
    "\n",
    "# Cleans the dictionary by only keeping unique values\n",
    "def dict_cleaner(dict):\n",
    "    unique_dict = {}\n",
    "    for key, values in dict.items():\n",
    "        unique_values = list(set(values))\n",
    "        unique_dict[key] = unique_values\n",
    "    return unique_dict\n",
    "\n",
    "\n",
    "# Consolidates all functions for clean processing\n",
    "def group_info(df, threshold = 85):\n",
    "    grouped_dataframe = group_similar_terms(df, threshold)\n",
    "    groups = dict_cleaner(produce_groups(df, threshold))\n",
    "    return grouped_dataframe, groups\n",
    "\n",
    "\n",
    "grouped_rising_queries, rising_group_dict = group_info(rising_queries.iloc[:, 2:], 85)\n",
    "grouped_top_queries, top_group_dict = group_info(top_queries.iloc[:, 2:], 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_unique_counts(df):\n",
    "    val_count1 = pd.value_counts(df.values.flatten())\n",
    "    filtered_val1 = val_count1[(val_count1 > 100) & (val_count1 < 2000)]\n",
    "\n",
    "    return filtered_val1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Grouped Data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rising_queries.to_csv(\"./cleaned_data/grouped_rising_queries.csv\", index = False)\n",
    "grouped_top_queries.to_csv(\"./cleaned_data/grouped_top_queries.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Group Dictionary to Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_dict_to_json(dictionary, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(dictionary, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rising_group_dict = dict_cleaner(rising_group_dict)\n",
    "top_group_dict = dict_cleaner(top_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dict_to_json(rising_group_dict, './cleaned_data/rising_group_dict.json')\n",
    "save_dict_to_json(top_group_dict, './cleaned_data/top_group_dict.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
