{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = pd.read_csv(\"./data/all_queries_8countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Desktop\\My Comp Sci Projects\\Diffusion_Research_Project\\diffusion_research\\data_cleaner3.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grouped_dataframe, groups\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m grouped_all_queries, all_group_dict \u001b[39m=\u001b[39m group_info(all_queries\u001b[39m.\u001b[39;49miloc[:, \u001b[39m3\u001b[39;49m:\u001b[39m6\u001b[39;49m], \u001b[39m90\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\andre\\Desktop\\My Comp Sci Projects\\Diffusion_Research_Project\\diffusion_research\\data_cleaner3.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroup_info\u001b[39m(df, threshold\u001b[39m=\u001b[39m\u001b[39m90\u001b[39m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     grouped_dataframe \u001b[39m=\u001b[39m group_similar_terms(df, threshold)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     groups \u001b[39m=\u001b[39m produce_groups(df, threshold)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grouped_dataframe, groups\n",
      "\u001b[1;32mc:\\Users\\andre\\Desktop\\My Comp Sci Projects\\Diffusion_Research_Project\\diffusion_research\\data_cleaner3.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m group_terms \u001b[39m=\u001b[39m []  \u001b[39m# List to store terms in the group\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m terms:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     similarity_score \u001b[39m=\u001b[39m fuzz\u001b[39m.\u001b[39;49mtoken_sort_ratio(query, term)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m similarity_score \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/My%20Comp%20Sci%20Projects/Diffusion_Research_Project/diffusion_research/data_cleaner3.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         found_group \u001b[39m=\u001b[39m group\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:105\u001b[0m, in \u001b[0;36mtoken_sort_ratio\u001b[1;34m(s1, s2, force_ascii, full_process)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoken_sort_ratio\u001b[39m(s1, s2, force_ascii\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, full_process\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    102\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a measure of the sequences' similarity between 0 and 100\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m    but sorting the token before comparing.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m _token_sort(s1, s2, partial\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, force_ascii\u001b[39m=\u001b[39;49mforce_ascii, full_process\u001b[39m=\u001b[39;49mfull_process)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\utils.py:38\u001b[0m, in \u001b[0;36mcheck_for_none.<locals>.decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m args[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m args[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:98\u001b[0m, in \u001b[0;36m_token_sort\u001b[1;34m(s1, s2, partial, force_ascii, full_process)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m partial_ratio(sorted1, sorted2)\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m ratio(sorted1, sorted2)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\utils.py:38\u001b[0m, in \u001b[0;36mcheck_for_none.<locals>.decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m args[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m args[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\utils.py:29\u001b[0m, in \u001b[0;36mcheck_for_equivalence.<locals>.decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m args[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m args[\u001b[39m1\u001b[39m]:\n\u001b[0;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m100\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\utils.py:47\u001b[0m, in \u001b[0;36mcheck_empty_string.<locals>.decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(args[\u001b[39m1\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 47\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:28\u001b[0m, in \u001b[0;36mratio\u001b[1;34m(s1, s2)\u001b[0m\n\u001b[0;32m     25\u001b[0m s1, s2 \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mmake_type_consistent(s1, s2)\n\u001b[0;32m     27\u001b[0m m \u001b[39m=\u001b[39m SequenceMatcher(\u001b[39mNone\u001b[39;00m, s1, s2)\n\u001b[1;32m---> 28\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mintr(\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m m\u001b[39m.\u001b[39;49mratio())\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\difflib.py:619\u001b[0m, in \u001b[0;36mSequenceMatcher.ratio\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mratio\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    598\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a measure of the sequences' similarity (float in [0,1]).\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \n\u001b[0;32m    600\u001b[0m \u001b[39m    Where T is the total number of elements in both sequences, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[39m    1.0\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m     matches \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(triple[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m triple \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_matching_blocks())\n\u001b[0;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m _calculate_ratio(matches, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb))\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\difflib.py:454\u001b[0m, in \u001b[0;36mSequenceMatcher.get_matching_blocks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mwhile\u001b[39;00m queue:\n\u001b[0;32m    453\u001b[0m     alo, ahi, blo, bhi \u001b[39m=\u001b[39m queue\u001b[39m.\u001b[39mpop()\n\u001b[1;32m--> 454\u001b[0m     i, j, k \u001b[39m=\u001b[39m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_longest_match(alo, ahi, blo, bhi)\n\u001b[0;32m    455\u001b[0m     \u001b[39m# a[alo:i] vs b[blo:j] unknown\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[39m# a[i:i+k] same as b[j:j+k]\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[39m# a[i+k:ahi] vs b[j+k:bhi] unknown\u001b[39;00m\n\u001b[0;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m k:   \u001b[39m# if k is 0, there was no matching block\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\difflib.py:379\u001b[0m, in \u001b[0;36mSequenceMatcher.find_longest_match\u001b[1;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[0;32m    377\u001b[0m j2lenget \u001b[39m=\u001b[39m j2len\u001b[39m.\u001b[39mget\n\u001b[0;32m    378\u001b[0m newj2len \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 379\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m b2j\u001b[39m.\u001b[39mget(a[i], nothing):\n\u001b[0;32m    380\u001b[0m     \u001b[39m# a[i] matches b[j]\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     \u001b[39mif\u001b[39;00m j \u001b[39m<\u001b[39m blo:\n\u001b[0;32m    382\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "common_words_to_exclude = [\"feminism\", \"what\"]\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def calculate_similarity(query, term):\n",
    "    # Calculate similarity using spaCy's similarity score\n",
    "    query_doc = nlp(query)\n",
    "    term_doc = nlp(term)\n",
    "    similarity_score = query_doc.similarity(term_doc)\n",
    "    return similarity_score\n",
    "\n",
    "def group_similar_terms(df, threshold=0.90):\n",
    "    term_groups = {col: {} for col in df.columns}\n",
    "\n",
    "    for col in df.columns:\n",
    "        for query in df[col]:\n",
    "            found_group = None\n",
    "\n",
    "            for group, terms in term_groups[col].items():\n",
    "                common_word_count = 0\n",
    "\n",
    "                for term in terms:\n",
    "                    similarity_score = calculate_similarity(query, term)\n",
    "                    if similarity_score >= threshold:\n",
    "                        found_group = group\n",
    "                        break\n",
    "\n",
    "                    # Split the terms into words and check for common words\n",
    "                    query_str = str(query)\n",
    "                    term_str = str(term)\n",
    "                    query_words = set(query_str.lower().split())\n",
    "                    term_words = set(term_str.lower().split())\n",
    "\n",
    "                    # Calculate the number of common words (excluding common_words_to_exclude)\n",
    "                    common_word_intersection = query_words.intersection(term_words)\n",
    "                    common_word_count = sum(1 for word in common_word_intersection if word not in common_words_to_exclude)\n",
    "\n",
    "                    if common_word_count >= 2:\n",
    "                        found_group = group\n",
    "                        break\n",
    "\n",
    "                if found_group:\n",
    "                    break\n",
    "\n",
    "            if found_group is None:\n",
    "                found_group = f'Group_({query})'\n",
    "                term_groups[col][found_group] = [query]\n",
    "            else:\n",
    "                term_groups[col][found_group].append(query)\n",
    "\n",
    "    grouped_data = {col: [term for group, terms in term_groups[col].items() for term in terms] for col in df.columns}\n",
    "    grouped_df = pd.DataFrame(grouped_data)\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "def produce_groups(df, threshold=0.90):\n",
    "    term_groups = defaultdict(list)\n",
    "\n",
    "    for col in df.columns:\n",
    "        for query in df[col]:\n",
    "            found_group = None\n",
    "\n",
    "            for group, terms in term_groups.items():\n",
    "                common_word_count = 0\n",
    "\n",
    "                for term in terms:\n",
    "                    similarity_score = calculate_similarity(query, term)\n",
    "                    if similarity_score >= threshold:\n",
    "                        found_group = group\n",
    "                        break\n",
    "\n",
    "                    # Split the terms into words and check for common words\n",
    "                    query_str = str(query)\n",
    "                    term_str = str(term)\n",
    "                    query_words = set(query_str.lower().split())\n",
    "                    term_words = set(term_str.lower().split())\n",
    "\n",
    "                    # Calculate the number of common words (excluding common_words_to_exclude)\n",
    "                    common_word_intersection = query_words.intersection(term_words)\n",
    "                    common_word_count = sum(1 for word in common_word_intersection if word not in common_words_to_exclude)\n",
    "\n",
    "                    if common_word_count >= 2:\n",
    "                        found_group = group\n",
    "                        break\n",
    "\n",
    "                if found_group:\n",
    "                    break\n",
    "\n",
    "            if found_group is None:\n",
    "                found_group = f'Group_({query})'\n",
    "                term_groups[found_group].append(query)\n",
    "            else:\n",
    "                term_groups[found_group].append(query)\n",
    "\n",
    "    grouped_terms_dict = {group: list(set(terms)) for group, terms in term_groups.items()}\n",
    "\n",
    "    return grouped_terms_dict\n",
    "\n",
    "def group_info(df, threshold=0.90):\n",
    "    grouped_dataframe = group_similar_terms(df, threshold)\n",
    "    groups = produce_groups(df, threshold)\n",
    "    return grouped_dataframe, groups\n",
    "\n",
    "# Example usage:\n",
    "grouped_all_queries, all_group_dict = group_info(all_queries.iloc[:, 3:6], 0.90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_unique_counts(df):\n",
    "    val_count1 = pd.value_counts(df.values.flatten())\n",
    "    filtered_val1 = val_count1[(val_count1 > 100) & (val_count1 < 2000)]\n",
    "\n",
    "    return filtered_val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_all_queries.to_csv(\"./cleaned_data/grouped_all_queries2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_dict_to_json(dictionary, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(dictionary, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans the dictionary by only keeping unique values\n",
    "def dict_cleaner(dict):\n",
    "    unique_dict = {}\n",
    "    for key, values in dict.items():\n",
    "        unique_values = list(set(values))\n",
    "        unique_dict[key] = unique_values\n",
    "    return unique_dict\n",
    "\n",
    "all_group_dict = dict_cleaner(all_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(all_group_dict, './cleaned_data/all_group_dict2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_group_df = pd.DataFrame(all_group_dict.items(), columns=[\"Grouped Term\", \"Related Queries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_group_df.to_csv(\"./cleaned_data/all_group_dictionary.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
